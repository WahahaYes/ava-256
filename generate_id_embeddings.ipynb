{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import ntpath\n",
    "import pickle\n",
    "from typing import Dict, Union\n",
    "\n",
    "import einops\n",
    "import torch\n",
    "import yaml\n",
    "from fvcore.common.config import CfgNode as CN\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data.ava_dataset import MultiCaptureDataset as AvaMultiCaptureDataset\n",
    "from data.ava_dataset import SingleCaptureDataset as AvaSingleCaptureDataset\n",
    "from data.ava_dataset import none_collate_fn\n",
    "from data.utils import MugsyCapture\n",
    "from utils import get_autoencoder, load_checkpoint, render_img, tocuda, train_csv_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_id_embedding(uid):\n",
    "    with open(f\"id_embeddings/{uid}.pickle\", \"rb\") as f:\n",
    "        id_embedding = pickle.load(f)\n",
    "    return id_embedding\n",
    "\n",
    "\n",
    "def id_cond_to_device(id_cond, device=torch.device(\"cuda\")):\n",
    "    # put id_cond on the gpu\n",
    "\n",
    "    id_cond2 = {}\n",
    "    id_cond2[\"z_tex\"] = id_cond[\"z_tex\"].detach().to(device)\n",
    "    id_cond2[\"z_geo\"] = id_cond[\"z_geo\"].detach().to(device)\n",
    "    id_cond2[\"b_tex\"] = [None, None, None, None, None, None, None, None]\n",
    "    id_cond2[\"b_geo\"] = [None, None, None, None, None, None, None, None]\n",
    "    for i in range(8):\n",
    "        id_cond2[\"b_tex\"][i] = id_cond[\"b_tex\"][i].detach().to(device)\n",
    "        id_cond2[\"b_geo\"][i] = id_cond[\"b_geo\"][i].detach().to(device)\n",
    "\n",
    "    return id_cond2\n",
    "\n",
    "\n",
    "def generate_image(ae, id_cond, cudadriver):\n",
    "    id_cond = id_cond_to_device(id_cond)\n",
    "\n",
    "    output = ae(\n",
    "        camrot=cudadriver[\"camrot\"],\n",
    "        campos=cudadriver[\"campos\"],\n",
    "        focal=cudadriver[\"focal\"],\n",
    "        princpt=cudadriver[\"princpt\"],\n",
    "        modelmatrix=cudadriver[\"modelmatrix\"],\n",
    "        avgtex=cudadriver[\"avgtex\"],\n",
    "        verts=cudadriver[\"verts\"],\n",
    "        neut_avgtex=cudadriver[\"neut_avgtex\"],\n",
    "        neut_verts=cudadriver[\"neut_verts\"],\n",
    "        target_neut_avgtex=None,\n",
    "        target_neut_verts=None,\n",
    "        id_cond=id_cond,\n",
    "        pixelcoords=cudadriver[\"pixelcoords\"],\n",
    "    )\n",
    "\n",
    "    rgb = output[\"irgbrec\"].detach().cpu().numpy()\n",
    "    rgb = einops.rearrange(rgb, \"1 c h w -> h w c\")\n",
    "\n",
    "    return rgb\n",
    "\n",
    "\n",
    "def render(ae, id_cond, cudadriver, out_path: str = \"test.png\"):\n",
    "    rgb = generate_image(ae, id_cond, cudadriver)\n",
    "    render_img([[rgb]], out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading single id captures: 100%|██████████| 256/256 [02:59<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@ Get autoencoder ABLATION CONFIG FILE : length of data set : 256\n",
      "dataset vertmean: (7306, 3)\n",
      "id_encoder params: 5062060\n",
      "encoder params: 5_551_232\n",
      "decoder params: 35_918_504\n",
      "colorcal params: 3_252\n",
      "bgmodel params: 454_739\n",
      "total params: 46_991_899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\github\\ava-256\\utils.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = th.load(filename)\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"aeparams_1440000.pt\"  # the pretrained model\n",
    "config = \"configs/config.yaml\"\n",
    "opts = []\n",
    "\n",
    "\n",
    "with open(config, \"r\") as file:\n",
    "    config = CN(yaml.load(file, Loader=yaml.UnsafeLoader))\n",
    "\n",
    "config.merge_from_list(opts)\n",
    "\n",
    "train_params = config.train\n",
    "\n",
    "# Train dataset mean/std texture and vertex for normalization\n",
    "train_captures, train_dirs = train_csv_loader(\n",
    "    train_params.dataset_dir, train_params.data_csv, train_params.nids\n",
    ")\n",
    "dataset = AvaMultiCaptureDataset(\n",
    "    train_captures, train_dirs, downsample=train_params.downsample\n",
    ")\n",
    "\n",
    "batchsize = 1\n",
    "numworkers = 1\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batchsize,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    num_workers=numworkers,\n",
    "    collate_fn=none_collate_fn,\n",
    ")\n",
    "\n",
    "# Get Autoencoder\n",
    "assetpath = \"assets\"\n",
    "ae = get_autoencoder(dataset, assetpath=assetpath)\n",
    "# Load from checkpoint\n",
    "ae = load_checkpoint(ae, checkpoint).cuda()\n",
    "# Set to Evaluation mode\n",
    "ae.eval()\n",
    "\n",
    "id_model = ae.id_encoder\n",
    "texmean = dataset.texmean\n",
    "vertmean = dataset.vertmean\n",
    "texstd = dataset.texstd\n",
    "vertstd = dataset.vertstd\n",
    "\n",
    "# Delete dataset because it is no longer used\n",
    "del dataset\n",
    "\n",
    "\n",
    "user_ids = []\n",
    "\n",
    "for ui in glob.glob(\"E://codec_dataset/*\"):\n",
    "    user_ids.append(ntpath.basename(ui))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [32:10<00:00,  7.54s/it]\n"
     ]
    }
   ],
   "source": [
    "n_people = 256\n",
    "all_uids = user_ids[0:n_people]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for uid in tqdm(all_uids):\n",
    "        # Driver capture dataloader\n",
    "        driver_capture = MugsyCapture(\n",
    "            mcd=uid.split(\"--\")[0], mct=uid.split(\"--\")[1], sid=uid.split(\"--\")[2]\n",
    "        )\n",
    "        driver_dir = f\"{train_params.dataset_dir}/{uid}/decoder\"\n",
    "        driver_dataset = AvaSingleCaptureDataset(\n",
    "            driver_capture, driver_dir, downsample=train_params.downsample\n",
    "        )\n",
    "\n",
    "        # Grab driven normalization stats\n",
    "        for dataset in [driver_dataset]:\n",
    "            dataset.texmean = texmean\n",
    "            dataset.texstd = texstd\n",
    "            dataset.vertmean = vertmean\n",
    "            dataset.vertstd = vertstd\n",
    "\n",
    "        # if possible, we want a front-facing camera view\n",
    "        if (\n",
    "            \"401031\" in driver_dataset.cameras\n",
    "            or \"401880\" in driver_dataset.cameras\n",
    "            or \"401878\" in driver_dataset.cameras\n",
    "        ):\n",
    "            driver_dataset.cameras = [\"401031\", \"401880\", \"401878\"]\n",
    "\n",
    "        driver_loader = torch.utils.data.DataLoader(\n",
    "            driver_dataset,\n",
    "            batch_size=batchsize,\n",
    "            shuffle=True,\n",
    "            drop_last=False,\n",
    "            num_workers=numworkers,\n",
    "            collate_fn=none_collate_fn,\n",
    "        )\n",
    "\n",
    "        for driver in driver_loader:\n",
    "            # Skip if any of the frames is empty\n",
    "            if driver is None:\n",
    "                continue\n",
    "\n",
    "            cudadriver: Dict[str, Union[torch.Tensor, int, str]] = tocuda(driver)\n",
    "\n",
    "            running_avg_scale = False\n",
    "            gt_geo = None\n",
    "            residuals_weight = 1.0\n",
    "            output_set = set([\"irgbrec\", \"bg\"])\n",
    "\n",
    "            id_embedding_dict = {}\n",
    "\n",
    "            id_cond = id_model(cudadriver[\"neut_verts\"], cudadriver[\"neut_avgtex\"])\n",
    "            id_embedding_dict[\"uid\"] = uid\n",
    "            id_embedding_dict[\"id_cond\"] = id_cond_to_device(\n",
    "                id_cond, torch.device(\"cpu\")\n",
    "            )\n",
    "            id_embedding_dict[\"cudadriver\"] = cudadriver\n",
    "\n",
    "            # serialize each embedding\n",
    "            with open(f\"id_embeddings/{uid}.pickle\", \"wb\") as f:\n",
    "                pickle.dump(id_embedding_dict, f)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [01:05<00:00,  3.93it/s]\n"
     ]
    }
   ],
   "source": [
    "for uid in tqdm(all_uids):\n",
    "    id_embedding = fetch_id_embedding(uid)\n",
    "    render(\n",
    "        ae,\n",
    "        id_embedding[\"id_cond\"],\n",
    "        id_embedding[\"cudadriver\"],\n",
    "        f\"out/{id_embedding['uid']}.png\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
