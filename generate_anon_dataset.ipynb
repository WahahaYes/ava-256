{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import ntpath\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import cv2\n",
    "import einops\n",
    "import numpy as np\n",
    "import scipy.special\n",
    "import torch\n",
    "import yaml\n",
    "from fvcore.common.config import CfgNode as CN\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data.ava_dataset import MultiCaptureDataset as AvaMultiCaptureDataset\n",
    "from data.ava_dataset import none_collate_fn\n",
    "from utils import get_autoencoder, load_checkpoint, render_img, train_csv_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_id_embedding(uid):\n",
    "    with open(f\"id_embeddings/{uid}.pickle\", \"rb\") as f:\n",
    "        id_embedding = pickle.load(f)\n",
    "    return id_embedding\n",
    "\n",
    "\n",
    "def id_cond_to_device(id_cond, device=torch.device(\"cuda\")):\n",
    "    # put id_cond on the gpu\n",
    "\n",
    "    id_cond2 = {}\n",
    "    id_cond2[\"z_tex\"] = id_cond[\"z_tex\"].detach().to(device)\n",
    "    id_cond2[\"z_geo\"] = id_cond[\"z_geo\"].detach().to(device)\n",
    "    id_cond2[\"b_tex\"] = [None, None, None, None, None, None, None, None]\n",
    "    id_cond2[\"b_geo\"] = [None, None, None, None, None, None, None, None]\n",
    "    for i in range(8):\n",
    "        id_cond2[\"b_tex\"][i] = id_cond[\"b_tex\"][i].detach().to(device)\n",
    "        id_cond2[\"b_geo\"][i] = id_cond[\"b_geo\"][i].detach().to(device)\n",
    "\n",
    "    return id_cond2\n",
    "\n",
    "\n",
    "def generate_image(ae, id_cond, cudadriver):\n",
    "    id_cond = id_cond_to_device(id_cond)\n",
    "\n",
    "    output = ae(\n",
    "        camrot=cudadriver[\"camrot\"],\n",
    "        campos=cudadriver[\"campos\"],\n",
    "        focal=cudadriver[\"focal\"],\n",
    "        princpt=cudadriver[\"princpt\"],\n",
    "        modelmatrix=cudadriver[\"modelmatrix\"],\n",
    "        avgtex=cudadriver[\"avgtex\"],\n",
    "        verts=cudadriver[\"verts\"],\n",
    "        neut_avgtex=cudadriver[\"neut_avgtex\"],\n",
    "        neut_verts=cudadriver[\"neut_verts\"],\n",
    "        target_neut_avgtex=None,\n",
    "        target_neut_verts=None,\n",
    "        id_cond=id_cond,\n",
    "        pixelcoords=cudadriver[\"pixelcoords\"],\n",
    "    )\n",
    "\n",
    "    rgb = output[\"irgbrec\"].detach().cpu().numpy()\n",
    "    rgb = einops.rearrange(rgb, \"1 c h w -> h w c\")\n",
    "\n",
    "    return rgb\n",
    "\n",
    "\n",
    "def increase_brightness(img, value=30):\n",
    "    img_start = np.copy(img)\n",
    "    img_grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    mask = img_grey < 1\n",
    "    mask = np.dstack([mask, mask, mask])\n",
    "\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "\n",
    "    lim = 255 - value\n",
    "    v[v > lim] = 255\n",
    "    v[v <= lim] += value\n",
    "\n",
    "    final_hsv = cv2.merge((h, s, v))\n",
    "    img = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
    "    img[mask] = img_start[mask]\n",
    "\n",
    "    final_img = (img_start / 255.0) / (1.004 - (img / 255.0))\n",
    "    final_img = 0.5 * final_img + (0.5 * (img / 255.0))\n",
    "\n",
    "    final_img[final_img > 1] = 1\n",
    "    final_img = (final_img * 255).astype(np.uint8)\n",
    "\n",
    "    return final_img\n",
    "\n",
    "\n",
    "def render(ae, id_cond, cudadriver, out_path: str = \"test.png\"):\n",
    "    rgb = generate_image(ae, id_cond, cudadriver)\n",
    "    rgb = increase_brightness(rgb, value=30)\n",
    "    render_img([[rgb]], out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading single id captures: 100%|██████████| 256/256 [01:37<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@ Get autoencoder ABLATION CONFIG FILE : length of data set : 256\n",
      "dataset vertmean: (7306, 3)\n",
      "id_encoder params: 5062060\n",
      "encoder params: 5_551_232\n",
      "decoder params: 35_918_504\n",
      "colorcal params: 3_252\n",
      "bgmodel params: 454_739\n",
      "total params: 46_991_899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\github\\ava-256\\utils.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = th.load(filename)\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"aeparams_1440000.pt\"  # the pretrained model\n",
    "config = \"configs/config.yaml\"\n",
    "opts = []\n",
    "\n",
    "\n",
    "with open(config, \"r\") as file:\n",
    "    config = CN(yaml.load(file, Loader=yaml.UnsafeLoader))\n",
    "\n",
    "config.merge_from_list(opts)\n",
    "\n",
    "train_params = config.train\n",
    "\n",
    "# Train dataset mean/std texture and vertex for normalization\n",
    "train_captures, train_dirs = train_csv_loader(\n",
    "    train_params.dataset_dir, train_params.data_csv, train_params.nids\n",
    ")\n",
    "dataset = AvaMultiCaptureDataset(\n",
    "    train_captures, train_dirs, downsample=train_params.downsample\n",
    ")\n",
    "\n",
    "batchsize = 1\n",
    "numworkers = 1\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batchsize,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    num_workers=numworkers,\n",
    "    collate_fn=none_collate_fn,\n",
    ")\n",
    "\n",
    "# Get Autoencoder\n",
    "assetpath = \"assets\"\n",
    "ae = get_autoencoder(dataset, assetpath=assetpath)\n",
    "# Load from checkpoint\n",
    "ae = load_checkpoint(ae, checkpoint).cuda()\n",
    "# Set to Evaluation mode\n",
    "ae.eval()\n",
    "\n",
    "id_model = ae.id_encoder\n",
    "texmean = dataset.texmean\n",
    "vertmean = dataset.vertmean\n",
    "texstd = dataset.texstd\n",
    "vertstd = dataset.vertstd\n",
    "\n",
    "# Delete dataset because it is no longer used\n",
    "del dataset\n",
    "\n",
    "\n",
    "user_ids = []\n",
    "\n",
    "for ui in glob.glob(\"E://codec_dataset/*\"):\n",
    "    user_ids.append(ntpath.basename(ui))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:38<00:00,  6.65it/s]\n"
     ]
    }
   ],
   "source": [
    "tex_embeddings = []\n",
    "geo_embeddings = []\n",
    "for uid in tqdm(user_ids):\n",
    "    id_embedding = fetch_id_embedding(uid)\n",
    "    tex_embeddings.append(id_embedding[\"id_cond\"][\"z_tex\"])\n",
    "    geo_embeddings.append(id_embedding[\"id_cond\"][\"z_geo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [03:44<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 8, 8])\n",
      "torch.Size([1, 128, 16, 16])\n",
      "torch.Size([1, 128, 32, 32])\n",
      "torch.Size([1, 64, 64, 64])\n",
      "torch.Size([1, 64, 128, 128])\n",
      "torch.Size([1, 32, 256, 256])\n",
      "torch.Size([1, 16, 512, 512])\n",
      "torch.Size([1, 3, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "avg_tex_bias = [[], [], [], [], [], [], [], []]\n",
    "avg_geo_bias = [[], [], [], [], [], [], [], []]\n",
    "for uid in tqdm(user_ids):\n",
    "    id_embedding = fetch_id_embedding(uid)\n",
    "    for i in range(8):\n",
    "        avg_tex_bias[i].append(id_embedding[\"id_cond\"][\"b_tex\"][i])\n",
    "        avg_geo_bias[i].append(id_embedding[\"id_cond\"][\"b_geo\"][i])\n",
    "\n",
    "for i in range(8):\n",
    "    avg_tex_bias[i] = torch.mean(torch.stack(avg_tex_bias[i]), dim=0)\n",
    "    avg_geo_bias[i] = torch.mean(torch.stack(avg_geo_bias[i]), dim=0)\n",
    "    print(avg_tex_bias[i].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save these embeddings to a file\n",
    "\n",
    "\n",
    "# tex_embeddings_numpy, geo_embeddings_numpy = [], []\n",
    "# for i in range(len(tex_embeddings)):\n",
    "#     tex_embeddings_numpy.append(tex_embeddings[i].cpu().detach().numpy())\n",
    "#     geo_embeddings_numpy.append(geo_embeddings[i].cpu().detach().numpy())\n",
    "# tex_embeddings_numpy = np.asarray(tex_embeddings_numpy)\n",
    "# geo_embeddings_numpy = np.asarray(geo_embeddings_numpy)\n",
    "\n",
    "# with open(\"tex_embeddings.pickle\", \"wb\") as f:\n",
    "#     pickle.dump(tex_embeddings_numpy, f)\n",
    "# with open(\"geo_embeddings.pickle\", \"wb\") as f:\n",
    "#     pickle.dump(geo_embeddings_numpy, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA to 16 dims\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pca = PCA(n_components=16)\n",
    "geo_pca = PCA(n_components=16)\n",
    "\n",
    "\n",
    "tex_np = np.asarray(tex_embeddings).reshape(len(user_ids), -1)\n",
    "scaler = StandardScaler()\n",
    "tex_np = scaler.fit_transform(tex_np)\n",
    "# tex_np = tex_np / np.linalg.norm(tex_np, axis=1, keepdims=True)\n",
    "\n",
    "pca_embeddings = pca.fit_transform(tex_np)\n",
    "# pca_embeddings = pca_embeddings / np.linalg.norm(pca_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "geo_np = np.asarray(geo_embeddings).reshape(len(user_ids), -1)\n",
    "geo_np = scaler.fit_transform(geo_np)\n",
    "geo_pca_embeddings = geo_pca.fit_transform(geo_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pca_embeddings.pickle\", \"wb\") as f:\n",
    "    pickle.dump(np.asarray(pca_embeddings), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anonymization import anonymize\n",
    "\n",
    "\n",
    "def generate_anon_embedding(\n",
    "    original_idx, pca_embeddings, epsilon: float = -1, theta: float = 0\n",
    "):\n",
    "    zs_tex, zs_geo = [], []\n",
    "    bs_tex, bs_geo = [[] for _ in range(8)], [[] for _ in range(8)]\n",
    "\n",
    "    A = anonymize(pca_embeddings[original_idx], epsilon=epsilon, theta=theta)\n",
    "\n",
    "    dists = []\n",
    "    for i in range(0, len(user_ids)):\n",
    "        B = pca_embeddings[i]\n",
    "        cos_sim = np.dot(A, B) / (np.linalg.norm(A) * np.linalg.norm(B))\n",
    "        dists.append(cos_sim)\n",
    "\n",
    "    sorted_ids = [x for _, x in sorted(zip(dists, user_ids), key=lambda pair: pair[0])]\n",
    "    dists = sorted(dists)\n",
    "\n",
    "    top5_ids = sorted_ids[-8:]\n",
    "    top5_dists = dists[-8:]\n",
    "    top5_dists = scipy.special.softmax(np.asarray(top5_dists) * 8)\n",
    "\n",
    "    for weight, uid in zip(top5_dists, top5_ids):\n",
    "        id_embedding = fetch_id_embedding(uid)\n",
    "        id_cond = id_embedding[\"id_cond\"]\n",
    "\n",
    "        zs_tex.append(weight * id_cond[\"z_tex\"])\n",
    "        zs_geo.append(weight * id_cond[\"z_geo\"])\n",
    "        for i in range(8):\n",
    "            bs_tex[i].append(weight * id_cond[\"b_tex\"][i])\n",
    "            bs_geo[i].append(weight * id_cond[\"b_geo\"][i])\n",
    "\n",
    "    new_id_cond = {}\n",
    "    new_id_cond[\"z_tex\"] = torch.sum(torch.stack(zs_tex), dim=0)\n",
    "    new_id_cond[\"z_geo\"] = torch.sum(torch.stack(zs_geo), dim=0)\n",
    "    new_id_cond[\"b_tex\"] = []\n",
    "    new_id_cond[\"b_geo\"] = []\n",
    "\n",
    "    for i in range(8):\n",
    "        new_id_cond[\"b_tex\"].append(torch.sum(torch.stack(bs_tex[i]), dim=0))\n",
    "        new_id_cond[\"b_geo\"].append(torch.sum(torch.stack(bs_geo[i]), dim=0))\n",
    "\n",
    "    return new_id_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anon_embedding_alternative(\n",
    "    original_idx,\n",
    "    pca_embeddings,\n",
    "    geo_pca_embeddings,\n",
    "    epsilon: float = -1,\n",
    "    theta: float = 0,\n",
    "):\n",
    "    zs_tex, zs_geo = [], []\n",
    "    bs_tex, bs_geo = [[] for _ in range(8)], [[] for _ in range(8)]\n",
    "\n",
    "    A = anonymize(pca_embeddings[original_idx], epsilon=epsilon, theta=theta)\n",
    "    G = anonymize(geo_pca_embeddings[original_idx], epsilon=epsilon, theta=theta)\n",
    "\n",
    "    dists = []\n",
    "    for i in range(0, len(user_ids)):\n",
    "        B = pca_embeddings[i]\n",
    "        cos_sim = np.dot(A, B) / (np.linalg.norm(A) * np.linalg.norm(B))\n",
    "        dists.append(cos_sim)\n",
    "\n",
    "    sorted_ids = [x for _, x in sorted(zip(dists, user_ids), key=lambda pair: pair[0])]\n",
    "    dists = sorted(dists)\n",
    "\n",
    "    top5_ids = sorted_ids[-8:]\n",
    "    top5_dists = dists[-8:]\n",
    "    top5_dists = scipy.special.softmax(np.asarray(top5_dists) * 8)\n",
    "\n",
    "    for weight, uid in zip(top5_dists, top5_ids):\n",
    "        id_embedding = fetch_id_embedding(uid)\n",
    "        id_cond = id_embedding[\"id_cond\"]\n",
    "\n",
    "        # zs_tex.append(weight * id_cond[\"z_tex\"])\n",
    "        # zs_geo.append(weight * id_cond[\"z_geo\"])\n",
    "        for i in range(8):\n",
    "            bs_tex[i].append(weight * id_cond[\"b_tex\"][i])\n",
    "            bs_geo[i].append(weight * id_cond[\"b_geo\"][i])\n",
    "\n",
    "    A = (A * np.std(pca_embeddings[original_idx])) + np.mean(\n",
    "        pca_embeddings[original_idx]\n",
    "    )\n",
    "    G = (G * np.std(geo_pca_embeddings[original_idx])) + np.mean(\n",
    "        geo_pca_embeddings[original_idx]\n",
    "    )\n",
    "\n",
    "    A = pca.inverse_transform(A)\n",
    "\n",
    "    ref_cond = fetch_id_embedding(user_ids[original_idx])[\"id_cond\"]\n",
    "\n",
    "    A = torch.tensor(\n",
    "        A.reshape(1, 16, 4, 4),\n",
    "        dtype=ref_cond[\"z_tex\"].dtype,\n",
    "        device=ref_cond[\"z_tex\"].device,\n",
    "    )\n",
    "    G = pca.inverse_transform(G)\n",
    "    G = torch.tensor(\n",
    "        G.reshape(1, 16, 4, 4),\n",
    "        dtype=ref_cond[\"z_tex\"].dtype,\n",
    "        device=ref_cond[\"z_tex\"].device,\n",
    "    )\n",
    "\n",
    "    new_id_cond = {}\n",
    "    new_id_cond[\"z_tex\"] = A\n",
    "    new_id_cond[\"z_geo\"] = G\n",
    "    new_id_cond[\"b_tex\"] = []\n",
    "    new_id_cond[\"b_geo\"] = []\n",
    "    # new_id_cond[\"b_tex\"] = avg_tex_bias\n",
    "    # new_id_cond[\"b_geo\"] = avg_geo_bias\n",
    "\n",
    "    for i in range(8):\n",
    "        new_id_cond[\"b_tex\"].append(torch.sum(torch.stack(bs_tex[i]), dim=0))\n",
    "        new_id_cond[\"b_geo\"].append(torch.sum(torch.stack(bs_geo[i]), dim=0))\n",
    "\n",
    "    return new_id_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 0\n",
      "test 1\n",
      "test 2\n",
      "test 3\n",
      "test 4\n",
      "test 5\n",
      "test 6\n",
      "test 7\n",
      "test 8\n",
      "test 9\n"
     ]
    }
   ],
   "source": [
    "# test alternate mechanism\n",
    "for j in range(10):\n",
    "    id_embedding = fetch_id_embedding(user_ids[j])\n",
    "    front_driver = id_embedding[\"cudadriver\"]\n",
    "\n",
    "    img_front = generate_image(ae, id_embedding[\"id_cond\"], front_driver)\n",
    "    img_row = []\n",
    "\n",
    "    print(f\"test {j}\")\n",
    "    for theta in [0, 90, 180]:\n",
    "        id_cond = generate_anon_embedding_alternative(\n",
    "            j, pca_embeddings, geo_pca_embeddings, theta=theta\n",
    "        )\n",
    "        img_gen = generate_image(ae, id_cond, front_driver)\n",
    "        img_gen = increase_brightness(img_gen)\n",
    "        img_row.append(img_gen)\n",
    "\n",
    "    render_img([img_row], f\"out/pca_recon_{j}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 0\n",
      "test 1\n",
      "test 2\n",
      "test 3\n",
      "test 4\n",
      "test 5\n",
      "test 6\n",
      "test 7\n",
      "test 8\n",
      "test 9\n"
     ]
    }
   ],
   "source": [
    "# set up some visualization tests\n",
    "for j in range(10):\n",
    "    id_embedding = fetch_id_embedding(user_ids[j])\n",
    "    front_driver = id_embedding[\"cudadriver\"]\n",
    "\n",
    "    img_front = generate_image(ae, id_embedding[\"id_cond\"], front_driver)\n",
    "\n",
    "    print(f\"test {j}\")\n",
    "    for theta in [0, 30, 60, 90, 120]:\n",
    "        id_cond = generate_anon_embedding(j, pca_embeddings, theta=theta)\n",
    "        img_gen = generate_image(ae, id_cond, front_driver)\n",
    "\n",
    "        render_img([[img_front, img_gen]], f\"out/anon_{j}_theta{theta}.png\")\n",
    "\n",
    "    for eps in [0, 1, 10, 100, 1000]:\n",
    "        id_cond = generate_anon_embedding(j, pca_embeddings, epsilon=eps)\n",
    "        img_gen = generate_image(ae, id_cond, front_driver)\n",
    "\n",
    "        render_img([[img_front, img_gen]], f\"out/anon_{j}_eps{eps}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 0\n",
      "test 1\n",
      "test 2\n",
      "test 3\n",
      "test 4\n",
      "test 5\n",
      "test 6\n",
      "test 7\n",
      "test 8\n",
      "test 9\n"
     ]
    }
   ],
   "source": [
    "# Visualizing IdentityDP applied to these features\n",
    "img_cols = []\n",
    "for j in range(10):\n",
    "    id_embedding = fetch_id_embedding(user_ids[j])\n",
    "    front_driver = id_embedding[\"cudadriver\"]\n",
    "\n",
    "    id_cond = id_embedding[\"id_cond\"]\n",
    "    img_front = generate_image(ae, id_embedding[\"id_cond\"], front_driver)\n",
    "    img_row = [img_front]\n",
    "\n",
    "    print(f\"test {j}\")\n",
    "    sens = 2\n",
    "    for mag in [1, 0.5, 0.2, 0.1]:\n",
    "        noisy_id_cond = id_cond.copy()\n",
    "        noisy_id_cond[\"z_tex\"] = id_cond[\"z_tex\"] + torch.tensor(\n",
    "            np.random.laplace(0, sens / mag, id_cond[\"z_tex\"].shape),\n",
    "            dtype=id_cond[\"z_tex\"].dtype,\n",
    "            device=id_cond[\"z_tex\"].device,\n",
    "        )\n",
    "        noisy_id_cond[\"z_geo\"] = id_cond[\"z_geo\"] + torch.tensor(\n",
    "            np.random.laplace(0, sens / mag, id_cond[\"z_geo\"].shape),\n",
    "            dtype=id_cond[\"z_tex\"].dtype,\n",
    "            device=id_cond[\"z_tex\"].device,\n",
    "        )\n",
    "        # for i in range(8):\n",
    "        #    noisy_id_cond[\"b_tex\"][i] = id_cond[\"b_tex\"][i] + torch.tensor(np.random.laplace(0, sens / mag, id_cond[\"b_tex\"][i].shape), dtype=id_cond[\"z_tex\"].dtype, device=id_cond[\"z_tex\"].device)\n",
    "        #    noisy_id_cond[\"b_geo\"][i] = id_cond[\"b_geo\"][i] + torch.tensor(np.random.laplace(0, sens / mag, id_cond[\"b_geo\"][i].shape), dtype=id_cond[\"z_tex\"].dtype, device=id_cond[\"z_tex\"].device)\n",
    "\n",
    "        img_gen = generate_image(ae, noisy_id_cond, front_driver)\n",
    "        img_row.append(img_gen)\n",
    "        render_img([img_row], f\"out/idp_test{j}.png\")\n",
    "    img_cols.append(img_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 0\n",
      "test 1\n",
      "test 2\n",
      "test 3\n",
      "test 4\n",
      "test 5\n",
      "test 6\n",
      "test 7\n",
      "test 8\n",
      "test 9\n"
     ]
    }
   ],
   "source": [
    "# Visualizing rotation with no PCA applied to these features\n",
    "\n",
    "\n",
    "def rot_feat(feat, eps, theta):\n",
    "    dims = feat.shape\n",
    "    feat = feat.view(1, -1)\n",
    "    feat = torch.tensor(\n",
    "        anonymize(feat.cpu().detach().numpy(), epsilon=eps, theta=theta),\n",
    "        device=feat.device,\n",
    "        dtype=feat.dtype,\n",
    "    )\n",
    "    # z_tex = torch.zeros_like(z_tex)\n",
    "    feat = feat.view(dims)\n",
    "    return feat\n",
    "\n",
    "\n",
    "img_cols = []\n",
    "for j in range(10):\n",
    "    id_embedding = fetch_id_embedding(user_ids[j])\n",
    "    front_driver = id_embedding[\"cudadriver\"]\n",
    "\n",
    "    id_cond = id_embedding[\"id_cond\"]\n",
    "    img_front = generate_image(ae, id_embedding[\"id_cond\"], front_driver)\n",
    "    img_row = [img_front]\n",
    "\n",
    "    print(f\"test {j}\")\n",
    "    eps = 0\n",
    "    for theta in [10]:  # , 20, 30]:\n",
    "        noisy_id_cond = id_cond.copy()\n",
    "        noisy_id_cond[\"z_tex\"] = rot_feat(noisy_id_cond[\"z_tex\"], eps, theta)\n",
    "        noisy_id_cond[\"z_geo\"] = rot_feat(noisy_id_cond[\"z_geo\"], eps, theta)\n",
    "\n",
    "        # for i in range(1):\n",
    "        #     try:\n",
    "        #         noisy_id_cond[\"b_tex\"][i] = rot_feat(noisy_id_cond[\"b_tex\"][i], eps, theta)\n",
    "        #         noisy_id_cond[\"b_geo\"][i] = rot_feat(noisy_id_cond[\"b_geo\"][i], eps, theta)\n",
    "        #     except:\n",
    "        #         print(f\"failed on {i}\")\n",
    "        #         break\n",
    "\n",
    "        img_gen = generate_image(ae, noisy_id_cond, front_driver)\n",
    "        img_row.append(img_gen)\n",
    "        render_img([img_row], f\"out/no_pca_test{j}.png\")\n",
    "    img_cols.append(img_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_img_cols = [list(row) for row in zip(*img_cols)]\n",
    "render_img(trans_img_cols, \"out/no_pca_test_all.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [2:29:46, 35.10s/it]\n"
     ]
    }
   ],
   "source": [
    "for j, uid in tqdm(enumerate(user_ids)):\n",
    "    id_embedding = fetch_id_embedding(uid)\n",
    "    # # already done\n",
    "    render(\n",
    "        ae,\n",
    "        id_embedding[\"id_cond\"],\n",
    "        id_embedding[\"cudadriver\"],\n",
    "        f\"D://github/FaceAnonEval/Datasets/codec/avatars/{id_embedding['uid']}.png\",\n",
    "    )\n",
    "\n",
    "    for theta in [30, 60, 90, 120, 150, 180]:\n",
    "        os.makedirs(\n",
    "            f\"D://github/FaceAnonEval/Anonymized Datasets/codec_theta{theta}/avatars\",\n",
    "            exist_ok=True,\n",
    "        )\n",
    "        id_cond = generate_anon_embedding(j, pca_embeddings, theta=theta)\n",
    "        render(\n",
    "            ae,\n",
    "            id_cond,\n",
    "            id_embedding[\"cudadriver\"],\n",
    "            f\"D://github/FaceAnonEval/Anonymized Datasets/codec_theta{theta}/avatars/{id_embedding['uid']}.png\",\n",
    "        )\n",
    "\n",
    "    for eps in [-1, 0, 1, 10, 50, 100, 500, 1000]:\n",
    "        os.makedirs(\n",
    "            f\"D://github/FaceAnonEval/Anonymized Datasets/codec_eps{eps}/avatars\",\n",
    "            exist_ok=True,\n",
    "        )\n",
    "        id_cond = generate_anon_embedding(j, pca_embeddings, epsilon=eps)\n",
    "        render(\n",
    "            ae,\n",
    "            id_cond,\n",
    "            id_embedding[\"cudadriver\"],\n",
    "            f\"D://github/FaceAnonEval/Anonymized Datasets/codec_eps{eps}/avatars/{id_embedding['uid']}.png\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [3:00:41, 42.35s/it]\n"
     ]
    }
   ],
   "source": [
    "# ALTERNATE DATASET\n",
    "os.makedirs(\n",
    "    \"D://github/FaceAnonEval/Datasets/codecpca/avatars\",\n",
    "    exist_ok=True,\n",
    ")\n",
    "\n",
    "for j, uid in tqdm(enumerate(user_ids)):\n",
    "    id_embedding = fetch_id_embedding(uid)\n",
    "    # already done\n",
    "    render(\n",
    "        ae,\n",
    "        id_embedding[\"id_cond\"],\n",
    "        id_embedding[\"cudadriver\"],\n",
    "        f\"D://github/FaceAnonEval/Datasets/codecpca/avatars/{id_embedding['uid']}.png\",\n",
    "    )\n",
    "\n",
    "    for theta in [30, 60, 90, 120, 150, 180]:\n",
    "        os.makedirs(\n",
    "            f\"D://github/FaceAnonEval/Anonymized Datasets/codecpca_theta{theta}/avatars\",\n",
    "            exist_ok=True,\n",
    "        )\n",
    "        id_cond = generate_anon_embedding_alternative(\n",
    "            j, pca_embeddings, geo_pca_embeddings, theta=theta\n",
    "        )\n",
    "        render(\n",
    "            ae,\n",
    "            id_cond,\n",
    "            id_embedding[\"cudadriver\"],\n",
    "            f\"D://github/FaceAnonEval/Anonymized Datasets/codecpca_theta{theta}/avatars/{id_embedding['uid']}.png\",\n",
    "        )\n",
    "\n",
    "    for eps in [-1, 0, 1, 10, 50, 100, 500, 1000]:\n",
    "        os.makedirs(\n",
    "            f\"D://github/FaceAnonEval/Anonymized Datasets/codecpca_eps{eps}/avatars\",\n",
    "            exist_ok=True,\n",
    "        )\n",
    "        id_cond = generate_anon_embedding_alternative(\n",
    "            j, pca_embeddings, geo_pca_embeddings, epsilon=eps\n",
    "        )\n",
    "        render(\n",
    "            ae,\n",
    "            id_cond,\n",
    "            id_embedding[\"cudadriver\"],\n",
    "            f\"D://github/FaceAnonEval/Anonymized Datasets/codecpca_eps{eps}/avatars/{id_embedding['uid']}.png\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20230405--1635--AAN112', '20230831--0814--ADL311', '20230324--0820--AEY864', '20230810--1355--AJR151', '20230810--1630--ANX726', '20230629--1159--APP152', '20230726--1657--AYE877', '20230308--1352--BDF920', '20220412--1326--BGR645', '20220711--1300--BHC106', '20230316--1103--BHK376', '20220809--1034--BJM420', '20230328--0800--BLY735', '20220815--1307--BMP511', '20220627--0812--BPM833', '20220819--0803--BUS250', '20230914--1105--BXQ083', '20210818--1332--CDR970', '20220831--0751--CMS162', '20230224--1359--CMZ386', '20230309--0820--CPA784', '20230901--1429--CPP930', '20230728--0757--CRV122', '20230720--1025--CTP175', '20230706--1016--CVR434', '20230306--0812--CYQ521', '20230615--1702--DAT749', '20230404--1217--DED409', '20230908--1645--DHA971', '20220614--1135--DNM410', '20210819--0903--DOT682', '20220808--0809--DPE040', '20230207--1619--DTW301', '20230511--0810--DUA525', '20230817--1136--DZS003', '20230303--0802--EFQ599', '20211006--0836--EID363', '20230801--1420--EJG940', '20230410--1641--ENU192', '20230816--1219--EPP420', '20230608--1209--EPY965', '20230915--0810--EVY126', '20230728--1002--EXA831', '20230310--1106--FCT871', '20230728--1203--FGK870', '20230601--1428--FJC651', '20230602--1453--FMB793', '20230728--1401--FPK790', '20230307--0805--FQC133', '20220315--1053--FWC239', '20210810--1306--FXN596', '20220815--1041--FZC097', '20230914--1035--GBH400', '20230317--1054--GBQ558', '20230313--0819--GCZ208', '20230404--0804--GJI283', '20230814--0822--GJL377', '20230814--0822--GTA798', '20220824--0804--GTU803', '20230410--1017--GVM433', '20230316--1356--GXR876', '20220815--0815--GZL442', '20230726--0805--GZU008', '20220811--1704--HAZ933', '20230906--0803--HBM931', '20230705--1657--HBT289', '20230503--1006--HDM935', '20220315--0818--HDZ165', '20220726--1314--HNU630', '20220214--0846--HOS732', '20230323--1044--HRO745', '20230630--1651--HTL154', '20230228--1358--HUJ848', '20230607--1409--HVO722', '20230815--0816--HWN858', '20230322--1706--IBQ026', '20220808--1036--IBX730', '20220802--0838--IDD857', '20230406--1655--IEM175', '20230328--1214--IEY203', '20210902--0817--IFG774', '20230525--1403--IHO061', '20230724--0814--IIQ590', '20230628--1405--IJB027', '20230308--1639--IKB041', '20230920--1404--INQ807', '20230809--1645--ITH925', '20230306--1402--IXG477', '20220712--1040--JEH262', '20230919--1056--JEN167', '20230612--1211--JFS610', '20230605--1354--JJA915', '20230905--1354--JOX535', '20220705--1055--JPG686', '20230515--0817--JUP244', '20230503--1400--JUP632', '20220114--0820--JZE802', '20220830--1711--JZN879', '20230802--1003--KAP399', '20210827--0906--KDA058', '20230213--0817--KGV285', '20230707--0817--KIX259', '20211001--0855--KJJ701', '20230124--1415--KKF424', '20220314--1116--KMD200', '20230516--0802--KOH994', '20220829--0836--KSA222', '20230531--1207--KTI779', '20230307--1648--KUG325', '20220812--1031--KVG225', '20230925--1644--KWL586', '20230725--1218--KXI621', '20230222--1639--KYL945', '20210901--0833--LAS440', '20230412--1218--LCJ763', '20231003--1715--LGV914', '20230330--0823--LMT312', '20230810--0819--LNL587', '20220901--1305--LNP057', '20230802--1205--LPB114', '20230814--1700--LVD531', '20230302--1654--LVY864', '20230809--0814--LWN501', '20230609--0959--LXC339', '20210929--0827--MCR809', '20230206--1354--MFS340', '20230925--0843--MGR740', '20230720--1415--MJX355', '20220815--1656--MOP211', '20230707--1200--MQL211', '20230719--1655--MVB233', '20230323--1656--MZL038', '20230804--1159--MZW030', '20220714--1031--NAB660', '20230908--1549--NCC597', '20220805--1649--NCU289', '20230810--1201--NDZ630', '20230601--1051--NEB896', '20230109--1327--NFS933', '20220311--1115--NFU480', '20230912--1313--NJI020', '20220304--1320--NNQ107', '20230502--1030--NOG724', '20210817--0900--NRE683', '20230918--1040--NTA876', '20230405--1213--NWM763', '20220819--1313--NYY338', '20230829--1404--OKX508', '20220215--0801--ONK705', '20230317--1648--ONK818', '20230502--0816--ORZ494', '20230725--1407--OWI326', '20230810--0812--OXS375', '20220407--0816--OYQ923', '20220510--1051--OZZ515', '20210928--0843--PAK800', '20230413--1031--PDG961', '20230817--0805--PDX936', '20230314--1632--PFN770', '20220406--1314--PGO261', '20230501--1015--PHE887', '20220401--0841--PHT202', '20230925--1356--PKH444', '20230717--0836--PSV686', '20230815--1401--PWG860', '20230607--0809--PZC599', '20230629--1004--QBN395', '20230106--1047--QCW581', '20230808--1656--QFA788', '20230322--0820--QHY185', '20230906--1059--QLL112', '20230717--1225--QNG975', '20230915--1648--QVU001', '20230811--1202--QWY097', '20230517--1211--QZP994', '20230518--1009--RFZ285', '20220307--1342--RGA575', '20230313--1653--RHL466', '20230919--1626--RHU956', '20230309--1343--RIM072', '20230530--1417--RKG241', '20230726--1016--RLB874', '20230327--1036--RSC756', '20230727--1211--RSJ093', '20230630--1424--RSY180', '20230502--1418--RWA092', '20230403--1022--RWS274', '20220426--1126--SCG995', '20230302--0808--SEI058', '20230627--1701--SIQ409', '20210929--1334--SKB942', '20220818--1653--SSF476', '20230728--1206--SVG441', '20220303--1247--TAY503', '20230807--0839--TCE049', '20230315--0816--TEA904', '20220202--0844--THN461', '20230526--1657--TLD140', '20220727--1059--TMA321', '20230602--0800--TOU998', '20220610--0901--TRP396', '20230227--1659--TRT983', '20230807--1011--TWH159', '20230410--0805--TWP078', '20230227--1052--TXE513', '20230816--0807--TYX812', '20230330--1421--UCD539', '20220711--1048--UGU718', '20210922--0839--UHV563', '20230515--1236--UII082', '20230918--1402--UIQ957', '20230315--1638--UIS049', '20230601--1208--UJD603', '20230627--1045--UMJ550', '20230407--1229--UMQ592', '20230224--1102--UPC002', '20230516--1240--UQD073', '20230804--1714--URP626', '20220809--1321--UTC375', '20230831--1037--UTI042', '20230302--1343--UWK768', '20230718--1021--UYF211', '20221213--1425--VBY735', '20220818--1304--VCN655', '20230301--1646--VNS883', '20230424--1024--VOG927', '20230131--1229--WBX393', '20220510--0837--WFU321', '20230721--1207--WOZ406', '20230914--1702--WOZ485', '20230710--0814--WWY595', '20230615--0808--WXT125', '20230508--1015--XGD109', '20230508--0828--XIK500', '20220218--0839--XIT043', '20210831--0815--XJT672', '20211001--1304--XMY093', '20220310--1336--XRZ540', '20230811--1636--XUD838', '20230608--1009--YIK625', '20230808--1157--YJF815', '20230808--0759--YLC760', '20230324--1647--YWZ007', '20230718--1434--YYA788', '20230512--1009--ZEL435', '20230915--1145--ZEM686', '20230320--0810--ZJN985', '20230303--1634--ZKN852', '20230323--1348--ZQK191', '20220310--1128--ZSC414', '20220216--1324--ZSE894', '20230607--1652--ZUE409', '20220831--1047--ZUS575', '20230703--0826--ZWG600', '20230421--1716--ZWR419', '20230324--1118--ZZS850']\n"
     ]
    }
   ],
   "source": [
    "print(user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the driver\n",
    "\n",
    "from typing import Dict, Union\n",
    "\n",
    "from data.ava_dataset import MultiCaptureDataset as AvaMultiCaptureDataset\n",
    "from data.ava_dataset import SingleCaptureDataset as AvaSingleCaptureDataset\n",
    "from data.utils import MugsyCapture\n",
    "from utils import tocuda\n",
    "\n",
    "\n",
    "def render_clip(target_idx, eps, theta, chosen_seg=None):\n",
    "    target_id = user_ids[target_idx]\n",
    "\n",
    "    os.makedirs(f\"figures/{target_id}/eps{eps}_theta{theta}\", exist_ok=True)\n",
    "\n",
    "    if eps < 0 and theta <= 0:\n",
    "        id_cond = fetch_id_embedding(target_id)[\"id_cond\"]\n",
    "    else:\n",
    "        id_cond = generate_anon_embedding(\n",
    "            target_idx, pca_embeddings, epsilon=eps, theta=theta\n",
    "        )\n",
    "\n",
    "    # Driver capture dataloader\n",
    "    driver_capture = MugsyCapture(\n",
    "        mcd=target_id.split(\"--\")[0],\n",
    "        mct=target_id.split(\"--\")[1],\n",
    "        sid=target_id.split(\"--\")[2],\n",
    "    )\n",
    "    driver_dir = f\"{train_params.dataset_dir}/{target_id}/decoder\"\n",
    "    driver_dataset = AvaSingleCaptureDataset(\n",
    "        driver_capture, driver_dir, downsample=train_params.downsample\n",
    "    )\n",
    "\n",
    "    seg_ids = np.unique(driver_dataset.framelist[\"seg_id\"])\n",
    "\n",
    "    if chosen_seg is None:\n",
    "        print(seg_ids)\n",
    "        chosen_seg = seg_ids[0]\n",
    "\n",
    "    if chosen_seg == \"last\":\n",
    "        chosen_seg = seg_ids[-1]\n",
    "\n",
    "    if chosen_seg == \"SEN\":\n",
    "        driver_dataset.framelist = driver_dataset.framelist.loc[\n",
    "            driver_dataset.framelist[\"seg_id\"].str.contains(\"SEN\")\n",
    "        ]\n",
    "    elif not chosen_seg == \"long\":\n",
    "        # select only desired segments\n",
    "        driver_dataset.framelist = driver_dataset.framelist.loc[\n",
    "            driver_dataset.framelist[\"seg_id\"] == chosen_seg\n",
    "        ]\n",
    "    else:\n",
    "        driver_dataset.framelist = driver_dataset.framelist.iloc[0:250]\n",
    "\n",
    "    if driver_dataset.framelist.values.tolist() == []:\n",
    "        raise ValueError(\n",
    "            f\"Asked to render Segment {chosen_seg}, but there are no frames with that Segment in {driver_capture}\"\n",
    "        )\n",
    "\n",
    "    if \"401031\" in driver_dataset.cameras:\n",
    "        driver_dataset.cameras = [\"401031\"]\n",
    "    elif \"401880\" in driver_dataset.cameras:\n",
    "        driver_dataset.cameras = [\"401880\"]\n",
    "    elif \"401878\" in driver_dataset.cameras:\n",
    "        driver_dataset.cameras = [\"401878\"]\n",
    "    else:\n",
    "        driver_dataset.cameras = [driver_dataset.cameras[0]]\n",
    "\n",
    "    driver_loader = torch.utils.data.DataLoader(\n",
    "        driver_dataset,\n",
    "        batch_size=batchsize,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=numworkers,\n",
    "        collate_fn=none_collate_fn,\n",
    "    )\n",
    "\n",
    "    it = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for driver in tqdm(driver_loader, desc=\"Rendering Frames\"):\n",
    "            # Skip if any of the frames is empty\n",
    "            if driver is None:\n",
    "                continue\n",
    "\n",
    "            cudadriver: Dict[str, Union[torch.Tensor, int, str]] = tocuda(driver)\n",
    "\n",
    "            img_gen = generate_image(ae, id_cond, cudadriver)\n",
    "            img_gen = increase_brightness(img_gen, value=30)\n",
    "\n",
    "            render_img(\n",
    "                [[img_gen]], f\"figures/{target_id}/eps{eps}_theta{theta}/{it}.png\"\n",
    "            )\n",
    "            it += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering Frames: 100%|██████████| 756/756 [03:47<00:00,  3.33it/s]\n"
     ]
    }
   ],
   "source": [
    "for z in [4]:\n",
    "    render_clip(z, eps=-1, theta=0, chosen_seg=\"SEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for z in range(10):\n",
    "#     for eps in [20]:  # [1, 5, 10, 20, 50, 100]:\n",
    "#         render_clip(z, eps=eps, theta=0, chosen_seg=\"long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering Frames: 100%|██████████| 557/557 [02:54<00:00,  3.19it/s]\n",
      "Rendering Frames: 100%|██████████| 557/557 [02:57<00:00,  3.14it/s]\n",
      "Rendering Frames: 100%|██████████| 557/557 [03:02<00:00,  3.05it/s]\n",
      "Rendering Frames: 100%|██████████| 756/756 [03:58<00:00,  3.17it/s]\n",
      "Rendering Frames: 100%|██████████| 756/756 [03:51<00:00,  3.27it/s]\n",
      "Rendering Frames: 100%|██████████| 756/756 [03:51<00:00,  3.26it/s]\n",
      "Rendering Frames: 100%|██████████| 582/582 [03:01<00:00,  3.21it/s]\n",
      "Rendering Frames: 100%|██████████| 582/582 [03:00<00:00,  3.23it/s]\n",
      "Rendering Frames: 100%|██████████| 582/582 [03:00<00:00,  3.22it/s]\n",
      "Rendering Frames: 100%|██████████| 1206/1206 [06:04<00:00,  3.31it/s]\n",
      "Rendering Frames: 100%|██████████| 1206/1206 [06:04<00:00,  3.31it/s]\n",
      "Rendering Frames: 100%|██████████| 1206/1206 [06:14<00:00,  3.22it/s]\n"
     ]
    }
   ],
   "source": [
    "for z in [0, 4, 6, 7]:\n",
    "    for theta in [45, 60, 70]:\n",
    "        render_clip(z, eps=-1, theta=theta, chosen_seg=\"SEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\github\\\\ava-256\\\\figures\\\\20230726--1657--AYE877\\\\eps-1_theta20\\\\0.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 21\u001b[0m\n\u001b[0;32m      9\u001b[0m inner_img \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m eps, theta \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# (1, 0),\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# (10, 0),\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m80\u001b[39m),\n\u001b[0;32m     19\u001b[0m ]:\n\u001b[0;32m     20\u001b[0m     img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[1;32m---> 21\u001b[0m         \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfigures/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43muser_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/eps\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43meps\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_theta\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtheta\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mframe\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     )\n\u001b[0;32m     23\u001b[0m     inner_img\u001b[38;5;241m.\u001b[39mappend(img)\n\u001b[0;32m     24\u001b[0m tot_img\u001b[38;5;241m.\u001b[39mappend(inner_img)\n",
      "File \u001b[1;32md:\\github\\ava-256\\.conda\\Lib\\site-packages\\PIL\\Image.py:3469\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3466\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[0;32m   3468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3469\u001b[0m     fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3470\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\github\\\\ava-256\\\\figures\\\\20230726--1657--AYE877\\\\eps-1_theta20\\\\0.png'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "tot_img = []\n",
    "\n",
    "# for z, frame in zip([1, 9, 5, 8], [0, 19, 5, 10]):\n",
    "for z, frame in zip([6, 5, 4, 8], [0, 19, 5, 10]):\n",
    "    user_id = user_ids[z]\n",
    "\n",
    "    inner_img = []\n",
    "    for eps, theta in [\n",
    "        # (1, 0),\n",
    "        # (10, 0),\n",
    "        # (50, 0),\n",
    "        (-1, 0),\n",
    "        (-1, 20),\n",
    "        (-1, 40),\n",
    "        (-1, 60),\n",
    "        (-1, 80),\n",
    "    ]:\n",
    "        img = np.asarray(\n",
    "            Image.open(f\"figures/{user_id}/eps{eps}_theta{theta}/{frame}.png\")\n",
    "        )\n",
    "        inner_img.append(img)\n",
    "    tot_img.append(inner_img)\n",
    "\n",
    "# if need to transpose: [list(i) for i in zip(*l)]\n",
    "render_img(tot_img, \"figures/avatar_results3.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 556/9999 [00:50<14:09, 11.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'D:\\\\github\\\\ava-256\\\\figures\\\\20230405--1635--AAN112\\\\eps-1_theta0\\\\556.png'\n",
      "a clip ended\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "try:\n",
    "    for frame in tqdm(range(9999)):\n",
    "        tot_img = []\n",
    "\n",
    "        # for z in [1, 2, 4, 8]:  # vid2\n",
    "        for z in [0, 4, 6, 7]:  # vid1\n",
    "            user_id = user_ids[z]\n",
    "\n",
    "            inner_img = []\n",
    "            for eps, theta in [\n",
    "                (-1, 0),\n",
    "                (-1, 45),\n",
    "                (-1, 60),\n",
    "                (-1, 70),\n",
    "            ]:\n",
    "                img = np.asarray(\n",
    "                    Image.open(f\"figures/{user_id}/eps{eps}_theta{theta}/{frame}.png\")\n",
    "                )\n",
    "                inner_img.append(img)\n",
    "            tot_img.append(inner_img)\n",
    "\n",
    "        # if need to transpose: [list(i) for i in zip(*l)]\n",
    "\n",
    "        render_img(tot_img, f\"figures/vid1/{frame}.jpg\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"a clip ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 0\n",
      "test 1\n",
      "test 2\n",
      "test 3\n",
      "test 4\n",
      "test 5\n",
      "test 6\n",
      "test 7\n",
      "test 8\n",
      "test 9\n"
     ]
    }
   ],
   "source": [
    "# render for figures\n",
    "\n",
    "\n",
    "img_col = []\n",
    "for j in range(10):\n",
    "    id_embedding = fetch_id_embedding(user_ids[j])\n",
    "    front_driver = id_embedding[\"cudadriver\"]\n",
    "\n",
    "    img_front = generate_image(ae, id_embedding[\"id_cond\"], front_driver)\n",
    "    img_row = [img_front]\n",
    "\n",
    "    print(f\"test {j}\")\n",
    "    for theta in [40, 50, 60, 70]:\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        id_cond = generate_anon_embedding(j, pca_embeddings, theta=theta)\n",
    "        img_gen = generate_image(ae, id_cond, front_driver)\n",
    "        img_row.append(img_gen)\n",
    "    img_col.append(img_row)\n",
    "\n",
    "render_img(img_col, \"gradual_change.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
